{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d69de08f",
   "metadata": {},
   "source": [
    "# <center>Speech Emotion Recognition Datasets test<center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847b8ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from matplotlib.pyplot import specgram\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import IPython.display as ipd  # To play sound in the notebook\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import soundfile\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# ignore warnings \n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc95ce63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for data\n",
    "SAVEE = \"surrey-audiovisual-expressed-emotion-savee/ALL/\"\n",
    "RAV = \"ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"\n",
    "TESS = \"toronto-emotional-speech-set-tess/TESS Toronto emotional speech set data/\"\n",
    "CREMA = \"cremad/AudioWAV/\"\n",
    "ANAD = \"Arabic Natural Audio Dataset/Speech/\"\n",
    "KSUEmotions = \"ksu_emotions/data/SPEECH/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933dd506",
   "metadata": {},
   "source": [
    "#  <center> About Surrey Audio-Visual Expressed Emotion (SAVEE) Dataset <center>\n",
    "The SAVEE database was recorded from four native English male speakers (identified as DC, JE, JK, KL), postgraduate students and researchers at the University of Surrey aged from 27 to 31 years. Emotion has been described psychologically in discrete categories: anger, disgust, fear, happiness, sadness and surprise. A neutral category is also added to provide recordings of 7 emotion categories.\n",
    "\n",
    "The text material consisted of 15 TIMIT sentences per emotion: 3 common, 2 emotion-specific and 10 generic sentences that were different for each emotion and phonetically-balanced. The 3 common and 2 × 6 = 12 emotion-specific sentences were recorded as neutral to give 30 neutral sentences. This resulted in a total of 120 utterances per speaker, for example:\n",
    "\n",
    "Common: She had your dark suit in greasy wash water all year. <br> \n",
    "Anger: Who authorized the unlimited expense account? <br>\n",
    "Disgust: Please take this dirty table cloth to the cleaners for me. <br> \n",
    "Fear: Call an ambulance for medical assistance. <br>\n",
    "Happiness: Those musicians harmonize marvelously. <br>\n",
    "Sadness: The prospect of cutting back spending is an unpleasant one for any governor. <br>\n",
    "Surprise: The carpet cleaners shampooed our oriental rug. <br>\n",
    "Neutral: The best way to learn is to solve extra problems.<br>\n",
    "#### Academic citation \n",
    "@inproceedings{Vlasenko_combiningframe,\n",
    "author = {Vlasenko, Bogdan and Schuller, Bjorn and Wendemuth, Andreas and Rigoll, Gerhard},\n",
    "year = {2007},\n",
    "month = {01},\n",
    "pages = {2249-2252},\n",
    "title = {Combining frame and turn-level information for robust recognition of emotions within speech},\n",
    "journal = {Proceedings of Interspeech}\n",
    "}\n",
    "#### Acquired from\n",
    "https://www.kaggle.com/datasets/ejlok1/surrey-audiovisual-expressed-emotion-savee "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7afae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data location for SAVEE\n",
    "dir_list = os.listdir(SAVEE)\n",
    "\n",
    "# parse the filename to get the emotions\n",
    "emotion=[]\n",
    "path = []\n",
    "for i in dir_list:\n",
    "    if i[-8:-6]=='_a':\n",
    "        emotion.append('angry')\n",
    "    elif i[-8:-6]=='_d':\n",
    "        emotion.append('disgust')\n",
    "    elif i[-8:-6]=='_f':\n",
    "        emotion.append('fear')\n",
    "    elif i[-8:-6]=='_h':\n",
    "        emotion.append('happy')\n",
    "    elif i[-8:-6]=='_n':\n",
    "        emotion.append('neutral')\n",
    "    elif i[-8:-6]=='sa':\n",
    "        emotion.append('sad')\n",
    "    elif i[-8:-6]=='su':\n",
    "        emotion.append('surprise')\n",
    "    else:\n",
    "        emotion.append('error') \n",
    "    path.append(SAVEE + i)\n",
    "\n",
    "# Now check out the label count distribution \n",
    "SAVEE_df = pd.DataFrame(emotion, columns = ['labels'])\n",
    "SAVEE_df['source'] = 'SAVEE'\n",
    "SAVEE_df = pd.concat([SAVEE_df, pd.DataFrame(path, columns = ['path'])], axis = 1)\n",
    "SAVEE_df.labels.value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "546932bb",
   "metadata": {},
   "source": [
    "# use the Librosa library \n",
    "fname = SAVEE + 'DC_f11.wav'  \n",
    "data, sampling_rate = librosa.load(fname)\n",
    "plt.figure(figsize=(15, 5))\n",
    "librosa.display.waveshow(data, sr=sampling_rate)\n",
    "\n",
    "# Lets play the audio \n",
    "ipd.Audio(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b87c3a",
   "metadata": {},
   "source": [
    "# <center> Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed99f8b",
   "metadata": {},
   "source": [
    "#### Files\n",
    "\n",
    "This portion of the RAVDESS contains 1440 files: 60 trials per actor x 24 actors = 1440. The RAVDESS contains 24 professional actors (12 female, 12 male), vocalizing two lexically-matched statements in a neutral North American accent. Speech emotions includes calm, happy, sad, angry, fearful, surprise, and disgust expressions. Each expression is produced at two levels of emotional intensity (normal, strong), with an additional neutral expression.\n",
    "\n",
    "#### File naming convention\n",
    "\n",
    "Each of the 1440 files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 03-01-06-01-02-01-12.wav). These identifiers define the stimulus characteristics:\n",
    "\n",
    "#### Filename identifiers\n",
    "\n",
    "Modality (01 = full-AV, 02 = video-only, 03 = audio-only).<br>\n",
    "Vocal channel (01 = speech, 02 = song).<br>\n",
    "Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).<br>\n",
    "Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.<br>\n",
    "Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").<br>\n",
    "Repetition (01 = 1st repetition, 02 = 2nd repetition).<br>\n",
    "Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).<br>\n",
    "Filename example: 03-01-06-01-02-01-12.wav<br>\n",
    "Audio-only (03)<br>\n",
    "Speech (01)<br>\n",
    "Fearful (06)<br>\n",
    "Normal intensity (01)<br>\n",
    "Statement \"dogs\" (02)<br>\n",
    "1st Repetition (01)<br>\n",
    "12th Actor (12)<br>\n",
    "Female, as the actor ID number is even.<br>\n",
    "<br>\n",
    "#### Academic citation \n",
    "Livingstone SR, Russo FA (2018) The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English. PLoS ONE 13(5): e0196391. https://doi.org/10.1371/journal.pone.0196391.\n",
    "\n",
    "#### Acquired from\n",
    "https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0519815d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list = os.listdir(RAV)\n",
    "dir_list.sort()\n",
    "\n",
    "\n",
    "emotion = []\n",
    "path = []\n",
    "for dir in dir_list:\n",
    "    # as their are 20 different actors in our previous directory we need to extract files for each actor.\n",
    "    actor = os.listdir(RAV + dir)\n",
    "    for file in actor:\n",
    "        part = file.split('.')[0]\n",
    "        part = part.split('-')\n",
    "        # third part in each file represents the emotion associated to that file.\n",
    "        emotion.append(int(part[2]))\n",
    "        path.append(RAV + dir + '/' + file)\n",
    "        \n",
    "# dataframe for emotion of files\n",
    "emotion_df = pd.DataFrame(emotion, columns=['labels'])\n",
    "\n",
    "# dataframe for path of files.\n",
    "path_df = pd.DataFrame(path, columns=['path'])\n",
    "RAV_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "RAV_df['source'] = 'RAVDESS'\n",
    "\n",
    "# changing integers to actual emotions.\n",
    "RAV_df.labels.replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\n",
    "RAV_df.labels.value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "76d274e8",
   "metadata": {},
   "source": [
    "# Pick a fearful track\n",
    "fname = RAV + 'Actor_14/03-01-06-02-02-02-14.wav'  \n",
    "data, sampling_rate = librosa.load(fname)\n",
    "plt.figure(figsize=(15, 5))\n",
    "librosa.display.waveshow(data, sr=sampling_rate)\n",
    "\n",
    "# Lets play the audio \n",
    "ipd.Audio(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26891a1",
   "metadata": {},
   "source": [
    "# <center>Toronto emotional speech set (TESS)<center>\n",
    "There are a set of 200 target words were spoken in the carrier phrase \"Say the word _' by two actresses (aged 26 and 64 years) and recordings were made of the set portraying each of seven emotions (anger, disgust, fear, happiness, pleasant surprise, sadness, and neutral). There are 2800 data points (audio files) in total.\n",
    "\n",
    "The dataset is organised such that each of the two female actor and their emotions are contain within its own folder. And within that, all 200 target words audio file can be found. The format of the audio file is a WAV format\n",
    "#### Academic citation \n",
    "“Toronto emotional speech set (TESS) | TSpace Repository.” https://tspace.library.utoronto.ca/handle/1807/24487 (accessed Sep. 11, 2022).\n",
    "#### Acquired from\n",
    "https://www.kaggle.com/datasets/ejlok1/toronto-emotional-speech-set-tess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c028a5ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dir_list = os.listdir(TESS)\n",
    "dir_list.sort()\n",
    "\n",
    "path = []\n",
    "emotion = []\n",
    "\n",
    "for i in dir_list:\n",
    "    fname = os.listdir(TESS + i)\n",
    "    for f in fname:\n",
    "        if i == 'OAF_angry' or i == 'YAF_angry':\n",
    "            emotion.append('angry')\n",
    "        elif i == 'OAF_disgust' or i == 'YAF_disgust':\n",
    "            emotion.append('disgust')\n",
    "        elif i == 'OAF_Fear' or i == 'YAF_fear':\n",
    "            emotion.append('fear')\n",
    "        elif i == 'OAF_happy' or i == 'YAF_happy':\n",
    "            emotion.append('happy')\n",
    "        elif i == 'OAF_neutral' or i == 'YAF_neutral':\n",
    "            emotion.append('neutral')                                \n",
    "        elif i == 'OAF_Pleasant_surprise' or i == 'YAF_pleasant_surprised':\n",
    "            emotion.append('surprise')               \n",
    "        elif i == 'OAF_Sad' or i == 'YAF_sad':\n",
    "            emotion.append('sad')\n",
    "        else:\n",
    "            emotion.append('Unknown')\n",
    "        path.append(TESS + i + \"/\" + f)\n",
    "\n",
    "TESS_df = pd.DataFrame(emotion, columns = ['labels'])\n",
    "TESS_df['source'] = 'TESS'\n",
    "TESS_df = pd.concat([TESS_df,pd.DataFrame(path, columns = ['path'])],axis=1)\n",
    "TESS_df.labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33636a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TESS_df.drop(TESS_df[TESS_df['labels']=='Unknown'].index, inplace=True)\n",
    "TESS_df.labels.value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e8978ef",
   "metadata": {},
   "source": [
    "# lets play a fearful track \n",
    "fname = TESS + 'YAF_fear/YAF_dog_fear.wav' \n",
    "\n",
    "data, sampling_rate = librosa.load(fname)\n",
    "plt.figure(figsize=(15, 5))\n",
    "librosa.display.waveshow(data, sr=sampling_rate)\n",
    "\n",
    "# Lets play the audio \n",
    "ipd.Audio(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0130e96",
   "metadata": {},
   "source": [
    "# <center> Crowd Sourced Emotional Multimodal Actors Dataset (CREMA-D) <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acd6b7e",
   "metadata": {},
   "source": [
    "CREMA-D is a data set of 7,442 original clips from 91 actors. These clips were from 48 male and 43 female actors between the ages of 20 and 74 coming from a variety of races and ethnicities (African America, Asian, Caucasian, Hispanic, and Unspecified). Actors spoke from a selection of 12 sentences. The sentences were presented using one of six different emotions (Anger, Disgust, Fear, Happy, Neutral, and Sad) and four different emotion levels (Low, Medium, High, and Unspecified).\n",
    "#### Academic citation \n",
    "Cao H, Cooper DG, Keutmann MK, Gur RC, Nenkova A, Verma R. CREMA-D: Crowd-sourced Emotional Multimodal Actors Dataset. IEEE Trans Affect Comput. 2014 Oct-Dec;5(4):377-390. doi: 10.1109/TAFFC.2014.2336244. PMID: 25653738; PMCID: PMC4313618.\n",
    "#### Acquired from\n",
    "https://www.kaggle.com/datasets/ejlok1/cremad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f40141",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list = os.listdir(CREMA)\n",
    "dir_list.sort()\n",
    "\n",
    "emotion = []\n",
    "path = []\n",
    "\n",
    "for wav in os.listdir(CREMA):\n",
    "    info = wav.partition(\".wav\")[0].split(\"_\")\n",
    "    if info[2] == 'SAD':\n",
    "        emotion.append(\"sad\")\n",
    "    elif info[2] == 'ANG':\n",
    "        emotion.append(\"angry\")\n",
    "    elif info[2] == 'DIS':\n",
    "        emotion.append(\"disgust\")\n",
    "    elif info[2] == 'FEA':\n",
    "        emotion.append(\"fear\")\n",
    "    elif info[2] == 'HAP':\n",
    "        emotion.append(\"happy\")\n",
    "    elif info[2] == 'NEU':\n",
    "        emotion.append(\"neutral\")\n",
    "    else:\n",
    "        emotion.append(\"unknown\")\n",
    "    path.append(CREMA + wav)\n",
    "\n",
    "    \n",
    "CREMA_df = pd.DataFrame(emotion, columns = ['labels'])\n",
    "CREMA_df['source'] = 'CREMA'\n",
    "CREMA_df = pd.concat([CREMA_df,pd.DataFrame(path, columns = ['path'])],axis=1)\n",
    "CREMA_df.labels.value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6fa8ac93",
   "metadata": {},
   "source": [
    "# use the well known Librosa library for this task \n",
    "fname = CREMA + '1012_IEO_HAP_HI.wav'  \n",
    "data, sampling_rate = librosa.load(fname)\n",
    "plt.figure(figsize=(15, 5))\n",
    "librosa.display.waveshow(data, sr=sampling_rate)\n",
    "\n",
    "# Lets play the audio \n",
    "ipd.Audio(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57191acb",
   "metadata": {},
   "source": [
    "# <center>Combine all the dataset's dataframe into one<center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350c234b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Eng = pd.concat([SAVEE_df, RAV_df, TESS_df, CREMA_df], axis = 0)\n",
    "print(df_Eng.labels.value_counts())\n",
    "df_Eng.to_csv(\"English_Data_path.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4a9031",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Eng.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8b193c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display number of rows, columns, etc.\n",
    "df_Eng.info()\n",
    "#or\n",
    "#df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33558cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's plot the count of each emotions in the english dataset.\n",
    "plt.figure(figsize =(21, 3))\n",
    "sns.countplot(df_Eng.labels)\n",
    "plt.ylabel('Count', size=12)\n",
    "plt.xlabel('Emotions', size=12)\n",
    "sns.despine(top=True, right=True, left=False, bottom=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ee5fc2",
   "metadata": {},
   "source": [
    "# <center> Arabic Natural Audio Dataset </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e997618a",
   "metadata": {},
   "source": [
    "the dataset has 3 discrete emotions: Happy,angry, and surprised. \n",
    "\n",
    "Eight videos of live calls between an anchor and a human outside the studio were downloaded from online Arabic talk shows. Each video was then divided into turns: callers and receivers. To label each video, 18 listeners were asked to listen to each video and select whether they perceive a happy, angry or surprised emotion. Silence, laughs and noisy chunks were removed. Every chunk was then automatically divided into 1 sec speech units forming our final corpus composed of 1384 records.\n",
    "\n",
    "\n",
    "#### Academic citation \n",
    "klaylat, Samira; Osman, ziad; Zantout, Rached; Hamandi, Lama (2018), “Arabic Natural Audio Dataset”, Mendeley Data, V1, doi: 10.17632/xm232yxf7t.1\n",
    "#### Acquired from\n",
    "https://data.mendeley.com/datasets/xm232yxf7t/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3efe5a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dir_list = os.listdir(ANAD)\n",
    "dir_list.sort()\n",
    "\n",
    "class_emotions = {'V1': 'happy', 'V2': 'surprise', 'V3': 'happy', 'V4': 'angry',\n",
    "        'V5': 'angry', 'V6': 'surprise', 'V7': 'angry','V8': 'happy'}\n",
    "\n",
    "emotion = []\n",
    "path = []\n",
    "\n",
    "\n",
    "\n",
    "for audio_file in dir_list:\n",
    "    for key in class_emotions:\n",
    "        if key in audio_file:\n",
    "            emotions = class_emotions[key]\n",
    "            emotion.append(emotions)\n",
    "    path.append(ANAD + audio_file)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "ANAD_df = pd.DataFrame(emotion, columns = ['labels'])\n",
    "ANAD_df['source'] = 'ANAD'\n",
    "ANAD_df = pd.concat([ANAD_df,pd.DataFrame(path, columns = ['path'])],axis=1)\n",
    "ANAD_df.labels.value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b56e238",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANAD_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d69ccb0",
   "metadata": {},
   "source": [
    "# <center> KSUEmotions </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076d0e2b",
   "metadata": {},
   "source": [
    "the dataset has 5 emotions: neutral, sadness, happiness, surprise, and questioning.\n",
    "the collection of data was devided into two pahses. Phase 1 had 10 female speakers and 10 male speakers which has 1596 records, while phase 2 has 7 female speakers and 7 male speakers that has 1680 records which makes the totoal of the data set 3276 records.\n",
    "\n",
    "#### File format\n",
    "The audio files are named using the DxxExxPgxxSxxTxx\n",
    "\n",
    "<img src=\"KSUE_file.png\"> \n",
    "\n",
    "#### Academic citation \n",
    "A. H. Meftah, M. A. Qamhan, Y. Seddiq, Y. A. Alotaibi and S. A. Selouani, \"King Saud University Emotions Corpus: Construction, Analysis, Evaluation, and Comparison,\" in IEEE Access, vol. 9, pp. 54201-54219, 2021, doi: 10.1109/ACCESS.2021.3070751. \n",
    "\n",
    "#### Acquired from \n",
    "https://catalog.ldc.upenn.edu/LDC2017S12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59f2bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list = os.listdir(KSUEmotions)\n",
    "dir_list.sort()\n",
    "\n",
    "class_emotions = {'E00': 'neutral', 'E01': 'happy', 'E02': 'sad', 'E03': 'surprise',\n",
    "                  'E04': 'questioning', 'E05': 'angry'}\n",
    "\n",
    "emotion = []\n",
    "path = []\n",
    "\n",
    "\n",
    "\n",
    "for phase in dir_list:\n",
    "    if os.path.isdir(os.path.join(KSUEmotions, phase)):\n",
    "        for emotion_path in os.listdir(os.path.join(KSUEmotions, phase)): \n",
    "            for audio_file in os.listdir(os.path.join(KSUEmotions, phase, emotion_path)):\n",
    "                path.append(os.path.join(KSUEmotions, phase, emotion_path, audio_file))\n",
    "                emotions = str(0)\n",
    "                for key in class_emotions:\n",
    "                    if key in audio_file:\n",
    "                        emotions = class_emotions[key]\n",
    "                        emotion.append(emotions)\n",
    "                        \n",
    "\n",
    "KSUEmotions_df = pd.DataFrame(emotion, columns = ['labels'])\n",
    "KSUEmotions_df['source'] = 'KSUEmotions'\n",
    "KSUEmotions_df = pd.concat([KSUEmotions_df,pd.DataFrame(path, columns = ['path'])],axis=1)\n",
    "KSUEmotions_df.labels.value_counts()                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39880fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "KSUEmotions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46207f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Arab = pd.concat([ANAD_df, KSUEmotions_df], axis = 0)\n",
    "df_Arab.labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c562b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Arab.to_csv(\"Arabic_Data_path.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6264b6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display number of rows, columns, etc.\n",
    "df_Arab.info()\n",
    "#or\n",
    "#df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c3fe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's plot the count of each emotions in the english dataset.\n",
    "plt.figure(figsize =(21, 3))\n",
    "sns.countplot(df_Arab.labels)\n",
    "plt.ylabel('Count', size=12)\n",
    "plt.xlabel('Emotions', size=12)\n",
    "sns.despine(top=True, right=True, left=False, bottom=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9b3ddd",
   "metadata": {},
   "source": [
    "# <center> All Datasets </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148be49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = pd.concat([df_Eng, df_Arab], axis = 0)\n",
    "data_path.labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee29ae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = data_path[data_path[\"labels\"]!=\"questioning\"]\n",
    "data_path = data_path[data_path[\"labels\"]!=\"disgust\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6c05d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path.to_csv(\"Data_path.csv\",index=False)\n",
    "data_path.labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de06544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display number of rows, columns, etc.\n",
    "data_path.info()\n",
    "#or\n",
    "#df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8149604",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's plot the count of each emotions in the english dataset.\n",
    "plt.figure(figsize =(21, 3))\n",
    "sns.countplot(data_path.labels)\n",
    "plt.ylabel('Count', size=12)\n",
    "plt.xlabel('Emotions', size=12)\n",
    "sns.despine(top=True, right=True, left=False, bottom=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004e2719",
   "metadata": {},
   "source": [
    "# <center> Features's extraction </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fb344c",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cebbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(data):\n",
    "    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n",
    "    data = data + noise_amp*np.random.normal(size=data.shape[0])\n",
    "    return data\n",
    "\n",
    "def stretch(data, rate=0.8):\n",
    "    return librosa.effects.time_stretch(data, rate)\n",
    "\n",
    "def shift(data):\n",
    "    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n",
    "    return np.roll(data, shift_range)\n",
    "\n",
    "def pitch(data, sampling_rate, pitch_factor=0.7):\n",
    "    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873cdb6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded56ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data, sample_rate, frame_length=2048, hop_length=512):\n",
    "    # ZCR\n",
    "    result = np.array([])\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n",
    "    result=np.hstack((result, zcr)) # stacking horizontally\n",
    "\n",
    "    # Chroma_stft\n",
    "    stft = np.abs(librosa.stft(data))\n",
    "    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
    "    result = np.hstack((result, chroma_stft)) # stacking horizontally\n",
    "\n",
    "    # MFCC\n",
    "    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)\n",
    "    result = np.hstack((result, mfcc)) # stacking horizontally\n",
    "\n",
    "    # Root Mean Square Value\n",
    "    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)\n",
    "    result = np.hstack((result, rms)) # stacking horizontally\n",
    "\n",
    "    # MelSpectogram\n",
    "    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)\n",
    "    result = np.hstack((result, mel)) # stacking horizontally\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_features(path):\n",
    "    # duration and offset are used to take care of the no audio in start and the ending of each audio files as seen above.\n",
    "    data, sample_rate = librosa.load(path, offset=0.4)\n",
    "    data = data.T\n",
    "    \n",
    "     # without augmentation\n",
    "    res1 = extract_features(data,sample_rate)\n",
    "    result = np.array(res1)\n",
    "    \n",
    "    # data with noise\n",
    "    data_noise = noise(data)\n",
    "    res2 = extract_features(data_noise,sample_rate)\n",
    "    result = np.vstack((result, res2)) # stacking vertically\n",
    "    \n",
    "    # data with pitching\n",
    "    data_pitch = pitch(data, sample_rate)\n",
    "    res3 = extract_features(data_pitch,sample_rate)\n",
    "    result = np.vstack((result, res3)) # stacking vertically\n",
    "    \n",
    "    # data with pitching and white_noise\n",
    "    new_data = pitch(data, sample_rate)\n",
    "    data_noise_pitch = noise(new_data)\n",
    "    res4 = extract_features(data_noise_pitch,sample_rate)\n",
    "    result = np.vstack((result, res4)) # stacking vertically\n",
    "    \n",
    "    # data with stretch\n",
    "    data_stretch = stretch(data)\n",
    "    res5 = extract_features(data_stretch,sample_rate)\n",
    "    result = np.vstack((result, res5)) # stacking vertically\n",
    "    \n",
    "    # data with pitch shifting\n",
    "    new_data = pitch(data, sample_rate)\n",
    "    data_shift_pitch = shift(new_data)\n",
    "    res6 = extract_features(data_shift_pitch,sample_rate)\n",
    "    result = np.vstack((result, res6)) # stacking vertically\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7fd20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = [], []\n",
    "i = 0\n",
    "for path, emotion in zip(data_path.path, data_path.labels):\n",
    "    feature = get_features(path)\n",
    "    #data, sample_rate = librosa.load(path, offset=0.6) in get_features()\n",
    "    for ele in feature:\n",
    "        X.append(ele)\n",
    "        # appending emotion 5 times as we have made 5 augmentation techniques on each audio file.\n",
    "        Y.append(emotion)\n",
    "        i+=1\n",
    "        if i%500==0:\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb9af32",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X), len(Y), data_path.path.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d7e489",
   "metadata": {},
   "outputs": [],
   "source": [
    "Features_Augmentation = pd.DataFrame(X)\n",
    "Features_Augmentation['labels'] = Y\n",
    "Features_Augmentation.to_csv('features_augmentation.csv', index=False)\n",
    "Features_Augmentation.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8970bb62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8170ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
